"""
è®¨è®ºç¼–æ’æ¨¡å— / Discussion Orchestration Module

åŠŸèƒ½è¯´æ˜ / Functionality:
è¿™ä¸ªæ¨¡å—æ˜¯æ•´ä¸ªåº”ç”¨çš„æ ¸å¿ƒï¼Œè´Ÿè´£ç¼–æ’å¤šæ¨¡å‹è®¨è®ºçš„æµç¨‹ï¼ŒåŒ…æ‹¬è½®æ¬¡ç®¡ç†ã€æ¶ˆæ¯å†å²ç»´æŠ¤ç­‰ã€‚
This module is the core of the application, responsible for orchestrating
the multi-model discussion process, including round management, message history maintenance, etc.

å®ç°ç»†èŠ‚ / Implementation Details:
- ç®¡ç†æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç«‹å¯¹è¯å†å²
- åè°ƒå¤šè½®è®¨è®ºæµç¨‹
- å¤„ç†äººç±»ä»‹å…¥æŒ‡å¯¼
- ç”Ÿæˆæœ€ç»ˆæ€»ç»“
- Manages independent conversation history for each model
- Coordinates multi-round discussion process
- Handles human intervention guidance
- Generates final summary

è®¾è®¡ç†ç”± / Design Rationale:
å°†è®¨è®ºé€»è¾‘é›†ä¸­åœ¨ä¸€ä¸ªæ¨¡å—ä¸­ï¼Œä¾¿äºç†è§£å’Œç»´æŠ¤æ•´ä¸ªè®¨è®ºæµç¨‹ã€‚
Centralizing discussion logic in one module makes it easier to understand
and maintain the entire discussion process.

å·²çŸ¥é—®é¢˜ / Known Issues:
- æ—  / None

TODO:
- æ”¯æŒè®¨è®ºæš‚åœå’Œæ¢å¤
- æ”¯æŒè®¨è®ºåˆ†æ”¯
- æ·»åŠ è®¨è®ºè´¨é‡è¯„ä¼°
- Support discussion pause and resume
- Support discussion branching
- Add discussion quality assessment
"""

import logging
from typing import List, Dict, Any, Optional
from .config import Config
from .prompts import PromptTemplates
from .token_manager import TokenManager
from .api_client import APIClient
from .ui import UIManager
from .markdown_writer import MarkdownWriter


class DiscussionManager:
    """
    è®¨è®ºç®¡ç†å™¨ç±» / Discussion Manager Class

    è¿™ä¸ªç±»è´Ÿè´£æ•´ä¸ªè®¨è®ºæµç¨‹çš„ç¼–æ’å’Œç®¡ç†ã€‚
    This class is responsible for orchestrating and managing the entire discussion process.
    """

    def __init__(self, config: Config, chosen_models: List[Dict[str, str]], topic: str):
        """
        åˆå§‹åŒ–è®¨è®ºç®¡ç†å™¨ / Initialize Discussion Manager

        Args:
            config: é…ç½®å¯¹è±¡ / Configuration object
            chosen_models: é€‰ä¸­çš„æ¨¡å‹åˆ—è¡¨ / Selected model list
            topic: è®¨è®ºä¸»é¢˜ / Discussion topic
        """
        self.config = config
        self.chosen_models = chosen_models
        self.topic = topic
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶ / Initialize components
        self.prompts = PromptTemplates()
        self.token_manager = TokenManager(
            model_name=config.tiktoken_model,
            response_tokens=config.response_tokens
        )
        self.api_client = APIClient(
            base_url=config.base_api,
            api_key=config.api_key,
            temperature_min=config.temperature_min,
            temperature_max=config.temperature_max,
            max_workers=config.max_workers
        )
        self.ui = UIManager()

        # å‚ä¸è€…å­—ç¬¦ä¸² / Participants string
        self.participants_str = "ã€".join(m["id"] for m in chosen_models)

        # åˆå§‹åŒ– Markdown å†™å…¥å™¨ / Initialize Markdown writer
        self.md_writer = MarkdownWriter(
            output_dir=config.output_dir,
            topic=topic,
            participants=self.participants_str,
            max_tokens=config.max_tokens
        )

        # Token ç»Ÿè®¡ / Token statistics
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0

        # å¯¹è¯å†å² / Conversation history
        # æ¯ä¸ªæ¨¡å‹ç»´æŠ¤ç‹¬ç«‹çš„å†å²è®°å½•
        # Each model maintains independent history
        self.history: Dict[str, List[Dict[str, Any]]] = {}
        self._initialize_history()

        self.logger.info(
            "è®¨è®ºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ / Discussion manager initialized: "
            "topic=%s, models=%d",
            topic, len(chosen_models)
        )

    def _initialize_history(self) -> None:
        """
        åˆå§‹åŒ–å¯¹è¯å†å² / Initialize Conversation History

        ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç‹¬ç«‹çš„å†å²è®°å½•ï¼ŒåŒ…å« system æ¶ˆæ¯ã€‚
        Creates independent history for each model, including system message.
        """
        for model in self.chosen_models:
            model_id = model["id"]
            system_prompt = self.prompts.format_system_prompt(
                model_name=model_id,
                topic=self.topic,
                participants=self.participants_str
            )
            self.history[model_id] = [
                {"role": "system", "content": system_prompt}
            ]

        self.logger.debug(
            "åˆå§‹åŒ–äº† %d ä¸ªæ¨¡å‹çš„å¯¹è¯å†å² / Initialized conversation history for %d models",
            len(self.chosen_models), len(self.chosen_models)
        )

    def _build_others_text(self, last_responses: Dict[str, str], exclude_model_id: str) -> str:
        """
        æ„å»ºå…¶ä»–å‚ä¸è€…çš„å‘è¨€æ–‡æœ¬ / Build Other Participants' Text

        Args:
            last_responses: ä¸Šä¸€è½®çš„å“åº” / Last round responses
            exclude_model_id: è¦æ’é™¤çš„æ¨¡å‹ ID / Model ID to exclude

        Returns:
            str: æ ¼å¼åŒ–çš„å…¶ä»–å‚ä¸è€…å‘è¨€ / Formatted other participants' text
        """
        parts = []
        for model in self.chosen_models:
            model_id = model["id"]
            if model_id != exclude_model_id and model_id in last_responses:
                parts.append(f"---\nã€{model_id}ã€‘:\n{last_responses[model_id]}")

        return "\n\n".join(parts)

    def run_round(self, round_idx: int, total_rounds: int,
                  last_responses: Dict[str, str],
                  human_input: Optional[str] = None) -> Dict[str, str]:
        """
        è¿è¡Œä¸€è½®è®¨è®º / Run One Round of Discussion

        Args:
            round_idx: å½“å‰è½®æ¬¡ç´¢å¼• / Current round index
            total_rounds: æ€»è½®æ¬¡æ•° / Total number of rounds
            last_responses: ä¸Šä¸€è½®çš„å“åº” / Last round responses
            human_input: äººç±»æŒ‡å¯¼å†…å®¹ï¼ˆå¯é€‰ï¼‰/ Human guidance content (optional)

        Returns:
            Dict[str, str]: æœ¬è½®å„æ¨¡å‹çš„å“åº” / This round's responses from each model

        å®ç°è¯´æ˜ / Implementation Notes:
        1. ä¸ºæ¯ä¸ªæ¨¡å‹æ„å»ºæç¤ºè¯
        2. è£å‰ªå†å²ä»¥é€‚åº”ä¸Šä¸‹æ–‡é™åˆ¶
        3. å¹¶å‘è°ƒç”¨ API è·å–å“åº”
        4. æ›´æ–°å†å²è®°å½•
        5. æ¸²æŸ“å’Œä¿å­˜ç»“æœ
        1. Build prompts for each model
        2. Trim history to fit context limits
        3. Concurrently call API to get responses
        4. Update history
        5. Render and save results
        """
        responses = {}
        remaining = total_rounds - round_idx

        self.logger.info(
            "å¼€å§‹ç¬¬ %d/%d è½®è®¨è®º / Starting round %d/%d",
            round_idx, total_rounds, round_idx, total_rounds
        )

        # ===== æ„å»ºæç¤ºè¯ / Build Prompts =====
        requests = []
        for model in self.chosen_models:
            model_id = model["id"]

            # å¦‚æœæœ‰äººç±»æŒ‡å¯¼ï¼Œæ·»åŠ åˆ°å†å² / If human guidance, add to history
            if human_input:
                self.history[model_id].append({
                    "role": "user",
                    "content": self.prompts.format_human_guide_prompt(human_input)
                })

            # æ„å»ºæœ¬è½®æç¤ºè¯ / Build this round's prompt
            if round_idx == 1 and not last_responses:
                # é¦–è½® / First round
                prompt = self.prompts.format_first_round_prompt(
                    current_round=round_idx,
                    total_rounds=total_rounds,
                    remaining=remaining,
                    model_name=model_id,
                    topic=self.topic
                )
            else:
                # åç»­è½®æ¬¡ / Subsequent rounds
                others_text = self._build_others_text(last_responses, model_id)
                prompt = self.prompts.format_discussion_prompt(
                    current_round=round_idx,
                    total_rounds=total_rounds,
                    remaining=remaining,
                    others_text=others_text
                )

            self.history[model_id].append({"role": "user", "content": prompt})

            # è£å‰ªå†å² / Trim history
            self.history[model_id] = self.token_manager.trim_history(
                self.history[model_id],
                self.config.max_tokens
            )

            # å‡†å¤‡è¯·æ±‚ / Prepare request
            requests.append((
                self.history[model_id],
                model_id,
                self.config.response_tokens
            ))

        # ===== å¹¶å‘è°ƒç”¨ API / Concurrent API Calls =====
        results = self.api_client.get_batch_completions(requests)

        # ===== æ›´æ–°å†å²å’Œç»Ÿè®¡ / Update History and Statistics =====
        for model in self.chosen_models:
            model_id = model["id"]
            if model_id in results:
                content, pt, ct = results[model_id]
                responses[model_id] = content
                self.total_prompt_tokens += pt
                self.total_completion_tokens += ct

                # æ·»åŠ åˆ°å†å² / Add to history
                self.history[model_id].append({
                    "role": "assistant",
                    "content": content
                })

        # ===== æ¸²æŸ“å’Œä¿å­˜ / Render and Save =====
        round_label = f"ç¬¬ {round_idx}/{total_rounds} è½®"
        if human_input:
            round_label += " (å«äººç±»æŒ‡å¯¼)"

        self.ui.render_round_header(round_label)

        if human_input:
            self.ui.render_human_input(human_input)

        self.md_writer.add_round_header(round_label, human_input)

        for model in self.chosen_models:
            model_id = model["id"]
            content = responses.get(model_id, "[æ— å›å¤]")
            self.ui.render_response(model_id, content, round_label)
            self.md_writer.add_model_response(model_id, content)

        self.ui.render_stats(self.total_prompt_tokens, self.total_completion_tokens)
        self.md_writer.add_token_stats(self.total_prompt_tokens, self.total_completion_tokens)
        self.md_writer.save()

        self.logger.info(
            "ç¬¬ %d è½®è®¨è®ºå®Œæˆ / Round %d completed",
            round_idx, round_idx
        )

        return responses

    def run_summary(self) -> None:
        """
        è¿è¡Œæœ€ç»ˆæ€»ç»“ / Run Final Summary

        è®©æ¯ä¸ªæ¨¡å‹ç”Ÿæˆæœ€ç»ˆæ€»ç»“ã€‚
        Have each model generate a final summary.
        """
        self.logger.info("å¼€å§‹æœ€ç»ˆæ€»ç»“ / Starting final summary")

        self.ui.console.print()
        self.ui.console.print(
            self.ui.console.rule("[bold bright_magenta]ğŸ“ æœ€ç»ˆæ€»ç»“ / Final Summary[/]",
                                style="bright_magenta")
        )
        self.ui.console.print()

        self.md_writer.add_summary_header()

        # å‡†å¤‡è¯·æ±‚ / Prepare requests
        requests = []
        for model in self.chosen_models:
            model_id = model["id"]
            summary_prompt = self.prompts.format_summary_prompt()
            self.history[model_id].append({"role": "user", "content": summary_prompt})
            self.history[model_id] = self.token_manager.trim_history(
                self.history[model_id],
                self.config.max_tokens
            )
            requests.append((
                self.history[model_id],
                model_id,
                self.config.response_tokens
            ))

        # å¹¶å‘è°ƒç”¨ API / Concurrent API calls
        results = self.api_client.get_batch_completions(requests)

        # æ¸²æŸ“å’Œä¿å­˜ / Render and save
        for model in self.chosen_models:
            model_id = model["id"]
            if model_id in results:
                content, pt, ct = results[model_id]
                self.total_prompt_tokens += pt
                self.total_completion_tokens += ct
                self.ui.render_response(model_id, content, "æœ€ç»ˆæ€»ç»“")
                self.md_writer.add_model_response(model_id, content)

        self.logger.info("æœ€ç»ˆæ€»ç»“å®Œæˆ / Final summary completed")

    def run_discussion(self, initial_rounds: int) -> str:
        """
        è¿è¡Œå®Œæ•´çš„è®¨è®ºæµç¨‹ / Run Complete Discussion Process

        Args:
            initial_rounds: åˆå§‹è½®æ•° / Initial number of rounds

        Returns:
            str: Markdown æ–‡ä»¶è·¯å¾„ / Markdown file path

        å®ç°è¯´æ˜ / Implementation Notes:
        è¿™æ˜¯ä¸»è¦çš„è®¨è®ºå¾ªç¯ï¼Œæ”¯æŒï¼š
        - å¤šè½®è®¨è®º
        - åŠ¨æ€è¿½åŠ è½®æ¬¡
        - äººç±»ä»‹å…¥æŒ‡å¯¼
        - æœ€ç»ˆæ€»ç»“
        This is the main discussion loop, supporting:
        - Multiple rounds of discussion
        - Dynamic addition of rounds
        - Human intervention guidance
        - Final summary
        """
        # æ˜¾ç¤ºè®¨è®ºå¼€å§‹ä¿¡æ¯ / Display discussion start info
        self.ui.render_discussion_start(
            self.topic,
            self.participants_str,
            initial_rounds
        )

        cumulative_round = 0
        total_rounds = initial_rounds
        last_responses = {}

        # ä¸»å¾ªç¯ / Main loop
        while True:
            # è¿è¡Œå‰©ä½™çš„è½®æ¬¡ / Run remaining rounds
            batch_rounds = total_rounds - cumulative_round
            for _ in range(batch_rounds):
                cumulative_round += 1
                last_responses = self.run_round(
                    cumulative_round,
                    total_rounds,
                    last_responses
                )

            # è½®æ¬¡ç»“æŸ / Round end
            self.ui.console.print()
            self.ui.console.print(
                self.ui.console.rule("[bold yellow]è½®æ¬¡ç»“æŸ / Round End[/]", style="yellow")
            )
            self.ui.render_stats(self.total_prompt_tokens, self.total_completion_tokens)
            self.ui.console.print()

            # è¯¢é—®æ˜¯å¦ç»§ç»­ / Ask if continue
            if not self.ui.prompt_continue():
                break

            # è¿½åŠ è½®æ¬¡ / Add extra rounds
            extra = self.ui.prompt_extra_rounds()
            total_rounds = cumulative_round + extra

            # äººç±»æŒ‡å¯¼ / Human guidance
            human_input = self.ui.prompt_human_guidance()
            if human_input:
                cumulative_round += 1
                total_rounds = cumulative_round + extra - 1
                last_responses = self.run_round(
                    cumulative_round,
                    total_rounds,
                    last_responses,
                    human_input=human_input
                )

        # æœ€ç»ˆæ€»ç»“ / Final summary
        self.run_summary()

        # æ˜¾ç¤ºç»Ÿè®¡ / Display statistics
        self.ui.render_summary_table(
            cumulative_round,
            len(self.chosen_models),
            self.total_prompt_tokens,
            self.total_completion_tokens
        )

        # ä¿å­˜ç»Ÿè®¡åˆ° Markdown / Save statistics to Markdown
        self.md_writer.add_statistics_table(
            cumulative_round,
            len(self.chosen_models),
            self.total_prompt_tokens,
            self.total_completion_tokens
        )
        md_filename = self.md_writer.save()

        # æ˜¾ç¤ºç»“æŸä¿¡æ¯ / Display end info
        # è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„ / Get log file path
        log_path = f"{self.config.log_dir}/{self.topic[:50]}_{self.md_writer.filename.split('/')[-1].split('_')[0]}.log"
        self.ui.render_discussion_end(md_filename, log_path)

        self.logger.info(
            "è®¨è®ºæµç¨‹å®Œæˆ / Discussion process completed: "
            "rounds=%d, models=%d, total_tokens=%d",
            cumulative_round, len(self.chosen_models),
            self.total_prompt_tokens + self.total_completion_tokens
        )

        return md_filename
