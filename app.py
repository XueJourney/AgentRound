from dotenv import load_dotenv
import os
import sys
import openai
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import random
import tiktoken
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.table import Table
from rich.prompt import Prompt, IntPrompt, Confirm
from rich.rule import Rule
from rich.text import Text

# åŠ è½½é…ç½®
load_dotenv()
BASEAPI = os.getenv("BASE_API", "https://api.openai.com/v1")
APIKEY = os.getenv("API_KEY")
RESPONSE_TOKENS = int(os.getenv("RESPONSE_TOKENS", "2048"))
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "32000"))
TEMPERATURE_MIN = float(os.getenv("TEMPERATURE_MIN", "0.4"))
TEMPERATURE_MAX = float(os.getenv("TEMPERATURE_MAX", "1.2"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "5"))
INITIAL_ROUNDS = int(os.getenv("INITIAL_ROUNDS", "3"))
TOPIC = os.getenv("TOPIC", "")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "discussions")
LOG_DIR = os.getenv("LOG_DIR", "log")
TIKTOKEN_MODEL = os.getenv("TIKTOKEN_MODEL", "gpt-4o")

SYSTEM_PROMPT_TEMPLATE = os.getenv("SYSTEM_PROMPT", (
    "ä½ æ˜¯ {model_name}ï¼Œæ­£åœ¨å‚ä¸ä¸€åœºå¤šAIåœ†æ¡Œè®¨è®ºã€‚\n"
    "è®¨è®ºä¸»é¢˜ï¼šã€Œ{topic}ã€\n"
    "å‚ä¸è€…ï¼š{participants}\n\n"
    "è§„åˆ™ï¼š\n"
    "1. ä½ å¿…é¡»ä»¥è‡ªå·±çš„èº«ä»½å‘è¨€ï¼Œæœ‰ç‹¬ç«‹çš„ç«‹åœºå’Œæ€è€ƒè§’åº¦\n"
    "2. è®¤çœŸé˜…è¯»å…¶ä»–å‚ä¸è€…çš„è§‚ç‚¹ï¼Œå¯ä»¥èµåŒã€åé©³æˆ–è¡¥å……\n"
    "3. ç”¨æ¸…æ™°çš„é€»è¾‘å’Œè®ºæ®æ”¯æ’‘ä½ çš„è§‚ç‚¹\n"
    "4. é¿å…ç©ºæ³›çš„å¥—è¯ï¼Œç»™å‡ºæœ‰æ·±åº¦çš„åˆ†æ\n"
    "5. æ¯è½®å‘è¨€æ§åˆ¶åœ¨300å­—ä»¥å†…ï¼Œç²¾ç‚¼è¡¨è¾¾"
))

FIRST_ROUND_PROMPT = os.getenv("FIRST_ROUND_PROMPT", (
    "# Agent\n"
    "ã€ç¬¬ {current_round}/{total_rounds} è½® | å‰©ä½™ {remaining} è½®ã€‘\n\n"
    "è¯·ä½œä¸º {model_name} ç‡å…ˆå‘è¡¨ä½ å¯¹ã€Œ{topic}ã€çš„è§‚ç‚¹ã€‚\n"
    "è¦æ±‚ï¼šäº®æ˜ç«‹åœºï¼Œç»™å‡ºæ ¸å¿ƒè®ºç‚¹å’Œæ”¯æ’‘è®ºæ®ã€‚"
))

DISCUSSION_PROMPT = os.getenv("DISCUSSION_PROMPT", (
    "# Agent\n"
    "ã€ç¬¬ {current_round}/{total_rounds} è½® | å‰©ä½™ {remaining} è½®ã€‘\n\n"
    "ä»¥ä¸‹æ˜¯ä¸Šä¸€è½®å…¶ä»–å‚ä¸è€…çš„å‘è¨€ï¼š\n{others_text}\n"
    "è¯·å‚è€ƒä»¥ä¸Šè§‚ç‚¹ï¼Œç»§ç»­æ·±å…¥è®¨è®ºã€‚ä½ å¯ä»¥ï¼š\n"
    "- åé©³ä½ ä¸è®¤åŒçš„è§‚ç‚¹å¹¶ç»™å‡ºç†ç”±\n"
    "- è¡¥å……å…¶ä»–äººé—æ¼çš„è§’åº¦\n"
    "- åœ¨ä»–äººè§‚ç‚¹åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ¨æ¼”\n"
    "- ä¿®æ­£æˆ–å®Œå–„è‡ªå·±ä¹‹å‰çš„ç«‹åœº"
))

HUMAN_GUIDE_PROMPT = os.getenv("HUMAN_GUIDE_PROMPT", (
    "# Human\n"
    "ç”¨æˆ·ä»‹å…¥æŒ‡å¯¼ï¼š\n{human_input}\n\n"
    "è¯·æ ¹æ®ç”¨æˆ·çš„æŒ‡å¯¼è°ƒæ•´ä½ çš„è®¨è®ºæ–¹å‘å’Œé‡ç‚¹ã€‚"
))

SUMMARY_PROMPT = os.getenv("SUMMARY_PROMPT", (
    "# Agent\n"
    "ã€æœ€ç»ˆæ€»ç»“è½®ã€‘\n\n"
    "è®¨è®ºå³å°†ç»“æŸï¼Œè¯·æ€»ç»“ï¼š\n"
    "1. ä½ çš„æœ€ç»ˆç«‹åœº\n"
    "2. è®¨è®ºä¸­æœ€æœ‰ä»·å€¼çš„è§‚ç‚¹ï¼ˆåŒ…æ‹¬ä»–äººçš„ï¼‰\n"
    "3. ä»å­˜åœ¨çš„åˆ†æ­§æˆ–å¾…æ¢è®¨çš„é—®é¢˜"
))

if not APIKEY:
    print("API_KEY æœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
    sys.exit(1)

# ===== æ—¥å¿—é…ç½® =====
os.makedirs(LOG_DIR, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# æ§åˆ¶å° handler: WARNING åŠä»¥ä¸Š
console_handler = logging.StreamHandler(sys.stderr)
console_handler.setLevel(logging.WARNING)
console_handler.setFormatter(logging.Formatter(
    '%(asctime)s - %(levelname)s - %(message)s'
))
logger.addHandler(console_handler)

# æ–‡ä»¶ handler å»¶è¿Ÿåˆ›å»ºï¼ˆéœ€è¦ TOPIC ç¡®å®šåå‘½åï¼‰
file_handler = None

def init_file_logger(topic):
    global file_handler
    safe_topic = "".join(c if c.isalnum() or c in "_- " else "_" for c in topic)[:50]
    log_path = os.path.join(LOG_DIR, f"{safe_topic}_{timestamp}.log")
    file_handler = logging.FileHandler(log_path, encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))
    logger.addHandler(file_handler)
    logger.info("æ—¥å¿—æ–‡ä»¶: %s", log_path)
    return log_path

# Rich console
console = Console()

# æ¨¡å‹é…è‰²
MODEL_COLORS = [
    "cyan", "green", "yellow", "magenta", "blue",
    "red", "bright_cyan", "bright_green", "bright_yellow", "bright_magenta"
]
model_color_map = {}

def get_model_color(mid):
    if mid not in model_color_map:
        idx = len(model_color_map) % len(MODEL_COLORS)
        model_color_map[mid] = MODEL_COLORS[idx]
    return model_color_map[mid]

logger.info("BASE_API: %s", BASEAPI)
logger.info("API_KEY: %s***", APIKEY[:8])
logger.info("MAX_TOKENS: %s", MAX_TOKENS)
logger.info("RESPONSE_TOKENS: %s", RESPONSE_TOKENS)

# tiktoken
try:
    encoder = tiktoken.encoding_for_model(TIKTOKEN_MODEL)
except Exception:
    encoder = tiktoken.get_encoding("cl100k_base")

def count_tokens(messages):
    total = 0
    for msg in messages:
        total += 4
        total += len(encoder.encode(msg.get("content", "")))
    total += 2
    return total

def trim_history(messages, max_ctx):
    while count_tokens(messages) + RESPONSE_TOKENS > max_ctx and len(messages) > 2:
        for i, msg in enumerate(messages):
            if msg["role"] != "system":
                removed = messages.pop(i)
                logger.info("è£å‰ªæ¶ˆæ¯ [%s]: %s...", removed["role"], removed["content"][:40])
                break
        else:
            break
    return messages

# é…ç½®openai
client = openai.Client(base_url=BASEAPI, api_key=APIKEY)

# è·å–æ¨¡å‹åˆ—è¡¨
model_env = os.getenv("MODELS")
if model_env:
    MODELLIST = [{"id": m.strip()} for m in model_env.split(",") if m.strip()]
else:
    try:
        resp = client.models.list()
        MODELLIST = [{"id": m.id} for m in resp.data]
    except Exception as e:
        logger.warning("è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: %s", e)
        sys.exit(1)

if not MODELLIST:
    logger.warning("æ¨¡å‹åˆ—è¡¨ä¸ºç©º")
    sys.exit(1)

# é€‰æ‹©æ¨¡å‹
console.print()
table = Table(title="å¯ç”¨æ¨¡å‹", show_header=True, header_style="bold cyan")
table.add_column("åºå·", style="dim", width=6)
table.add_column("æ¨¡å‹åç§°", style="bold")
for i, m in enumerate(MODELLIST):
    table.add_row(str(i), m["id"])
console.print(table)

CHOSENMODEL = []
while True:
    try:
        idx = IntPrompt.ask("\nè¯·è¾“å…¥æ¨¡å‹åºå·")
        if 0 <= idx < len(MODELLIST):
            CHOSENMODEL.append(MODELLIST[idx])
            chosen_names = [m["id"] for m in CHOSENMODEL]
            console.print(f"  å·²é€‰æ‹©: [bold green]{', '.join(chosen_names)}[/]")
            if not Confirm.ask("ç»§ç»­é€‰æ‹©?", default=False):
                break
        else:
            console.print("[red]åºå·è¶…å‡ºèŒƒå›´[/]")
    except (ValueError, IndexError):
        console.print("[red]è¾“å…¥é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥[/]")

if not CHOSENMODEL:
    logger.warning("æœªé€‰æ‹©ä»»ä½•æ¨¡å‹")
    sys.exit(1)

if not TOPIC:
    TOPIC = Prompt.ask("\nè¯·è¾“å…¥è®¨è®ºä¸»é¢˜")
ROUND = INITIAL_ROUNDS or IntPrompt.ask("è¯·è¾“å…¥å¯¹è¯è½®æ•°")

# åˆå§‹åŒ–æ–‡ä»¶æ—¥å¿—
log_path = init_file_logger(TOPIC)

participants_str = "ã€".join(m["id"] for m in CHOSENMODEL)
total_prompt_tokens = 0
total_completion_tokens = 0

# ===== Markdown è®°å½• =====
os.makedirs(OUTPUT_DIR, exist_ok=True)
safe_topic = "".join(c if c.isalnum() or c in "_- " else "_" for c in TOPIC)[:50]
md_filename = os.path.join(OUTPUT_DIR, f"{timestamp}_{safe_topic}.md")

md_lines = []

def md_append(*lines):
    for line in lines:
        md_lines.append(line)

def save_markdown():
    with open(md_filename, "w", encoding="utf-8") as f:
        f.write("\n".join(md_lines))
    logger.info("å¯¹è¯è®°å½•å·²ä¿å­˜: %s", md_filename)

md_append(
    f"# ğŸ—£ï¸ å¤šæ¨¡å‹è®¨è®ºè®°å½•",
    f"",
    f"> **ä¸»é¢˜**: {TOPIC}  ",
    f"> **æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  ",
    f"> **å‚ä¸æ¨¡å‹**: {participants_str}  ",
    f"> **Token ä¸Šé™**: {MAX_TOKENS}",
    f"",
    f"---",
    f""
)

def get_response(messages, model_id):
    response = client.chat.completions.create(
        model=model_id,
        messages=messages,
        temperature=random.uniform(TEMPERATURE_MIN, TEMPERATURE_MAX),
        max_tokens=RESPONSE_TOKENS
    )
    content = response.choices[0].message.content
    usage = response.usage
    return content, usage.prompt_tokens, usage.completion_tokens

def build_system_prompt(model_id):
    return SYSTEM_PROMPT_TEMPLATE.format(
        model_name=model_id,
        topic=TOPIC,
        participants=participants_str
    )

def build_others_text(last_responses, exclude_mid):
    parts = []
    for other in CHOSENMODEL:
        omid = other["id"]
        if omid != exclude_mid and omid in last_responses:
            parts.append(f"---\nã€{omid}ã€‘:\n{last_responses[omid]}")
    return "\n\n".join(parts)

def render_response(mid, content, round_label):
    """ç”¨ Rich æ¸²æŸ“æ¨¡å‹å›å¤"""
    color = get_model_color(mid)
    panel = Panel(
        Markdown(content),
        title=f"[bold {color}]ğŸ¤– {mid}[/]",
        subtitle=f"[dim]{round_label}[/]",
        border_style=color,
        padding=(1, 2)
    )
    console.print(panel)

def render_human_input(text):
    panel = Panel(
        Text(text, style="bold white"),
        title="[bold bright_white]ğŸ§‘ Human æŒ‡å¯¼[/]",
        border_style="bright_white",
        padding=(1, 2)
    )
    console.print(panel)

def render_stats():
    total = total_prompt_tokens + total_completion_tokens
    console.print(
        f"  [dim]ğŸ“Š prompt: {total_prompt_tokens:,} | "
        f"completion: {total_completion_tokens:,} | "
        f"total: {total:,}[/]"
    )

def run_round(history, round_idx, total_rounds, last_responses, human_input=None):
    global total_prompt_tokens, total_completion_tokens
    responses = {}
    remaining = total_rounds - round_idx

    for model in CHOSENMODEL:
        mid = model["id"]

        if human_input:
            history[mid].append({
                "role": "user",
                "content": HUMAN_GUIDE_PROMPT.format(human_input=human_input)
            })

        if round_idx == 1 and not last_responses:
            history[mid].append({
                "role": "user",
                "content": FIRST_ROUND_PROMPT.format(
                    current_round=round_idx,
                    total_rounds=total_rounds,
                    remaining=remaining,
                    model_name=mid,
                    topic=TOPIC
                )
            })
        else:
            others_text = build_others_text(last_responses, mid)
            history[mid].append({
                "role": "user",
                "content": DISCUSSION_PROMPT.format(
                    current_round=round_idx,
                    total_rounds=total_rounds,
                    remaining=remaining,
                    others_text=others_text
                )
            })

        history[mid] = trim_history(history[mid], MAX_TOKENS)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_model = {
            executor.submit(get_response, history[m["id"]], m["id"]): m["id"]
            for m in CHOSENMODEL
        }
        for future in as_completed(future_to_model):
            mid = future_to_model[future]
            try:
                content, pt, ct = future.result()
                responses[mid] = content
                total_prompt_tokens += pt
                total_completion_tokens += ct
                logger.info("[%s] prompt=%d, completion=%d", mid, pt, ct)
            except Exception as e:
                logger.warning("[%s] è¯·æ±‚å¤±è´¥: %s", mid, e)
                responses[mid] = f"[è¯·æ±‚å¤±è´¥: {e}]"

    for model in CHOSENMODEL:
        mid = model["id"]
        if mid in responses:
            history[mid].append({"role": "assistant", "content": responses[mid]})

    # æ¸²æŸ“
    round_label = f"ç¬¬ {round_idx}/{total_rounds} è½®"
    if human_input:
        round_label += " (å«äººç±»æŒ‡å¯¼)"

    console.print()
    console.print(Rule(f"[bold]ğŸ“Œ {round_label}[/]", style="bright_blue"))
    console.print()

    if human_input:
        render_human_input(human_input)

    md_append(f"## ğŸ“Œ {round_label}", f"")
    if human_input:
        md_append(f"### ğŸ§‘ Human æŒ‡å¯¼", f"", f"> {human_input}", f"")

    for model in CHOSENMODEL:
        mid = model["id"]
        content = responses.get(mid, "[æ— å›å¤]")
        render_response(mid, content, round_label)
        md_append(f"### ğŸ¤– {mid}", f"", f"{content}", f"")

    render_stats()
    md_append(
        f"> ğŸ“Š ç´¯è®¡ tokens â€” prompt: {total_prompt_tokens:,}, completion: {total_completion_tokens:,}",
        f"", f"---", f""
    )
    save_markdown()

    return responses

# åˆå§‹åŒ– history
history = {}
for model in CHOSENMODEL:
    mid = model["id"]
    history[mid] = [{"role": "system", "content": build_system_prompt(mid)}]

# å¯åŠ¨æç¤º
console.print()
console.print(Rule("[bold bright_blue]ğŸ—£ï¸ å¤šæ¨¡å‹è®¨è®ºå¼€å§‹[/]", style="bright_blue"))
console.print(f"  ä¸»é¢˜: [bold]{TOPIC}[/]")
console.print(f"  æ¨¡å‹: [bold green]{participants_str}[/]")
console.print(f"  è½®æ•°: [bold]{ROUND}[/]")
console.print()

# ä¸»å¾ªç¯
cumulative_round = 0
total_rounds = ROUND
last_responses = {}

while True:
    batch_rounds = total_rounds - cumulative_round
    for r in range(batch_rounds):
        cumulative_round += 1
        last_responses = run_round(
            history, cumulative_round, total_rounds, last_responses
        )

    console.print()
    console.print(Rule("[bold yellow]è½®æ¬¡ç»“æŸ[/]", style="yellow"))
    render_stats()
    console.print()

    if not Confirm.ask("æ˜¯å¦å¼€å¯æ–°çš„è½®æ¬¡?", default=False):
        # æœ€ç»ˆæ€»ç»“
        console.print()
        console.print(Rule("[bold bright_magenta]ğŸ“ æœ€ç»ˆæ€»ç»“[/]", style="bright_magenta"))
        console.print()

        md_append(f"## ğŸ“ æœ€ç»ˆæ€»ç»“", f"")

        for model in CHOSENMODEL:
            mid = model["id"]
            history[mid].append({"role": "user", "content": SUMMARY_PROMPT})
            history[mid] = trim_history(history[mid], MAX_TOKENS)

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_model = {
                executor.submit(get_response, history[m["id"]], m["id"]): m["id"]
                for m in CHOSENMODEL
            }
            for future in as_completed(future_to_model):
                mid = future_to_model[future]
                try:
                    content, pt, ct = future.result()
                    total_prompt_tokens += pt
                    total_completion_tokens += ct
                    render_response(mid, content, "æœ€ç»ˆæ€»ç»“")
                    md_append(f"### ğŸ¤– {mid}", f"", f"{content}", f"")
                except Exception as e:
                    logger.warning("[%s] æ€»ç»“å¤±è´¥: %s", mid, e)
                    md_append(f"### ğŸ¤– {mid}", f"", f"[æ€»ç»“å¤±è´¥: {e}]", f"")

        # ç»Ÿè®¡è¡¨æ ¼
        stats_table = Table(title="ğŸ“Š è®¨è®ºç»Ÿè®¡", show_header=True, header_style="bold cyan")
        stats_table.add_column("æŒ‡æ ‡", style="bold")
        stats_table.add_column("æ•°å€¼", justify="right")
        stats_table.add_row("æ€»è½®æ•°", str(cumulative_round))
        stats_table.add_row("å‚ä¸æ¨¡å‹", str(len(CHOSENMODEL)))
        stats_table.add_row("Prompt Tokens", f"{total_prompt_tokens:,}")
        stats_table.add_row("Completion Tokens", f"{total_completion_tokens:,}")
        stats_table.add_row("æ€» Tokens", f"{total_prompt_tokens + total_completion_tokens:,}")
        console.print()
        console.print(stats_table)

        md_append(
            f"---", f"",
            f"## ğŸ“Š ç»Ÿè®¡", f"",
            f"| æŒ‡æ ‡ | æ•°å€¼ |",
            f"|------|------|",
            f"| æ€»è½®æ•° | {cumulative_round} |",
            f"| å‚ä¸æ¨¡å‹ | {len(CHOSENMODEL)} |",
            f"| Prompt Tokens | {total_prompt_tokens:,} |",
            f"| Completion Tokens | {total_completion_tokens:,} |",
            f"| æ€» Tokens | {total_prompt_tokens + total_completion_tokens:,} |",
            f""
        )
        save_markdown()

        console.print()
        console.print(f"  ğŸ“„ å¯¹è¯è®°å½•: [link={md_filename}]{md_filename}[/]")
        console.print(f"  ğŸ“‹ è¿è¡Œæ—¥å¿—: [link={log_path}]{log_path}[/]")
        console.print()
        console.print(Rule("[bold bright_blue]è®¨è®ºç»“æŸ[/]", style="bright_blue"))
        break

    extra = IntPrompt.ask("è¿½åŠ å‡ è½®")
    total_rounds = cumulative_round + extra

    human_input = Prompt.ask("æœ‰éœ€è¦æŒ‡å¯¼çš„æ–¹å‘å—? (ç›´æ¥å›è½¦è·³è¿‡)", default="").strip()
    if human_input:
        cumulative_round += 1
        total_rounds = cumulative_round + extra - 1
        last_responses = run_round(
            history, cumulative_round, total_rounds, last_responses,
            human_input=human_input
        )